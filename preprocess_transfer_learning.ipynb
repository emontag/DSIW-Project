{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wd = os.path.join(os.getcwd(),'test')\n",
    "test_files = [f for f in listdir(train_wd) if isfile(join(train_wd, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuples = list(zip(df[\"landmark_id\"], df[\"id\"]))\n",
    "lm_dict = defaultdict(list)\n",
    "for tup in tuples:\n",
    "    lm_dict[tup[0]].append(tup[1])\n",
    "\n",
    "#lm_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img_size = (32,32)\n",
    "for i in tqdm([*lm_dict]):\n",
    "    filename = str(i) + \".jpg\"\n",
    "    if filename in downloaded_files:\n",
    "        p=os.path.join(train_wd,filename)\n",
    "        img=cv2.imread(p)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        imwrite(filename,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer 10% of data in train to a new folder called val\n",
    "val_wd=os.path.join(os.getcwd(),'val')\n",
    "if not exists(val_wd):\n",
    "    makedirs(val_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files = downloaded_files[:len(downloaded_files)//100]\n",
    "train_files = downloaded_files[len(downloaded_files)//100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm([*lm_dict]): \n",
    "    newpath = os.path.join(train_wd, str(key))\n",
    "    valpath = os.path.join(val_wd, str(key))\n",
    "\n",
    "    for index, img in enumerate(lm_dict[key]):\n",
    "        if index >= 50:\n",
    "            break\n",
    "        \n",
    "        filename = str(img) + \".jpg\"\n",
    "        if exists(os.path.join(train_wd, filename)) and filename in train_files:\n",
    "            if not exists(newpath):\n",
    "                makedirs(newpath)\n",
    "#             try:\n",
    "#                 img=Image.open(os.path.join(train_wd, filename))\n",
    "#                 img=img.convert('RGB')\n",
    "#             except Exception:\n",
    "#                 print (\"Bad one here:\", os.path.join(train_wd, filename))\n",
    "#                 os.remove(os.path.join(train_wd, filename))\n",
    "#                 continue\n",
    "            shutil.move(os.path.join(train_wd, filename), os.path.join(newpath, filename))\n",
    "        elif exists(os.path.join(train_wd, filename)) and filename in val_files:\n",
    "            if not exists(valpath):\n",
    "                makedirs(valpath)\n",
    "#             try:\n",
    "#                 img=Image.open(os.path.join(train_wd, filename))\n",
    "#                 img=img.convert('RGB')\n",
    "#             except Exception:\n",
    "#                 print (\"Bad one here:\", os.path.join(train_wd, filename))\n",
    "#                 os.remove(os.path.join(train_wd, filename))\n",
    "#                 continue\n",
    "            shutil.move(os.path.join(train_wd, filename), os.path.join(valpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[*lm_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.move(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in tqdm([*lm_dict]):\n",
    "#     newpath = os.path.join(train_wd, str(key))\n",
    "# #     print(newpath)\n",
    "#     valpath = os.path.join(val_wd, str(key))\n",
    "#     path=None\n",
    "#     if not exists(os.path.join(newpath)):\n",
    "#         path = valpath\n",
    "#     else:\n",
    "#         path = newpath\n",
    "\n",
    "#     for img in lm_dict[key]:\n",
    "#         filename = str(img) + \".jpg\"\n",
    "#         if exists(os.path.join(path, filename)) and filename in train_files:\n",
    "#             try:\n",
    "#                 img=Image.open(os.path.join(path, filename))\n",
    "#                 img=img.convert('RGB')\n",
    "#             except Exception:\n",
    "#                 print (\"Bad one here:\", os.path.join(path, filename))\n",
    "#                 os.remove(os.path.join(path, filename))\n",
    "#                 continue\n",
    "#         elif exists(os.path.join(path, filename)) and filename in val_files:\n",
    "#             try:\n",
    "#                 img=Image.open(os.path.join(path, filename))\n",
    "#                 img=img.convert('RGB')\n",
    "#             except Exception:\n",
    "#                 print (\"Bad one here:\", os.path.join(path, filename))\n",
    "#                 os.remove(os.path.join(path, filename))\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in downloaded_files:\n",
    "    filename = os.path.join(os.getcwd(),\"train\",filename)\n",
    "    try:\n",
    "        pil_im = Image.open(filename, 'r')\n",
    "#         plt.imshow(np.asarray(pil_im))\n",
    "#         plt.show()\n",
    "    except Exception:\n",
    "        print(filename, \"removed\")\n",
    "        os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #/tmp/imagenet/classify_image_graph_def.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_graph(model_file):\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def read_tensor_from_image_file(file_name, input_height=299, input_width=299,input_mean=0, input_std=255):\n",
    "    input_name = \"file_reader\"\n",
    "    output_name = \"normalized\"\n",
    "    file_reader = tf.read_file(file_name, input_name)\n",
    "    if file_name.endswith(\".png\"):\n",
    "        image_reader = tf.image.decode_png(file_reader, channels = 3,\n",
    "                                       name='png_reader')\n",
    "    elif file_name.endswith(\".gif\"):\n",
    "        image_reader = tf.squeeze(tf.image.decode_gif(file_reader,\n",
    "                                                  name='gif_reader'))\n",
    "    elif file_name.endswith(\".bmp\"):\n",
    "        image_reader = tf.image.decode_bmp(file_reader, name='bmp_reader')\n",
    "    else:\n",
    "        image_reader = tf.image.decode_jpeg(file_reader, channels = 3,\n",
    "                                        name='jpeg_reader')\n",
    "    float_caster = tf.cast(image_reader, tf.float32)\n",
    "    dims_expander = tf.expand_dims(float_caster, 0);\n",
    "    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
    "    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
    "    sess = tf.Session()\n",
    "    result = sess.run(normalized)\n",
    "\n",
    "    return result\n",
    "\n",
    "def load_labels(label_file):\n",
    "    label = []\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n",
    "    for l in proto_as_ascii_lines:\n",
    "        label.append(l.rstrip())\n",
    "    return label\n",
    "\n",
    "file_name = \"./test/2eec3e1e3e745237.jpg\"\n",
    "model_file = \"./output.pb\"\n",
    "label_file = \"./labels.txt\"\n",
    "input_height = 299\n",
    "input_width = 299\n",
    "input_mean = 128\n",
    "input_std = 128\n",
    "input_layer = \"Mul\"\n",
    "output_layer = \"final_result\"\n",
    "\n",
    "graph = load_graph(model_file)\n",
    "submission = open(\"submissions_2.csv\", \"w\")\n",
    "submission.write(\"id,landmarks\\n\")\n",
    "\n",
    "for file_name in tqdm(test_files[2000:4000]):\n",
    "    t = read_tensor_from_image_file(\"./test/\" + file_name,\n",
    "                                  input_height=input_height,\n",
    "                                  input_width=input_width,\n",
    "                                  input_mean=input_mean,\n",
    "                                  input_std=input_std)\n",
    "\n",
    "    input_name = \"import/\" + input_layer\n",
    "    output_name = \"import/\" + output_layer\n",
    "    input_operation = graph.get_operation_by_name(input_name);\n",
    "    output_operation = graph.get_operation_by_name(output_name);\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        results = sess.run(output_operation.outputs[0],\n",
    "                          {input_operation.outputs[0]: t})\n",
    "    results = np.squeeze(results)\n",
    "    \n",
    "    top_k = results.argsort()[-5:][::-1]\n",
    "    labels = load_labels(label_file)\n",
    "    \n",
    "    submission.write(str(file_name[:-4]) + \",\" + str(labels[top_k[0]]) + \" \" + str(results[top_k[0]]) + \"\\n\")\n",
    "    \n",
    "submission.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
